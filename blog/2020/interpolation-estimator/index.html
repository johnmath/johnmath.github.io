<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reproducing 'Does data interpolation contradict statistical optimality?' | John Abascal</title> <meta name="author" content="John Abascal"/> <meta name="description" content=""/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://johnmath.github.io/blog/2020/interpolation-estimator/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">John </span>Abascal</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">research</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reproducing 'Does data interpolation contradict statistical optimality?'</h1> <p class="post-meta">May 21, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fas fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/machine"> <i class="fas fa-tag fa-sm"></i> machine</a>   <a href="/blog/category/learning"> <i class="fas fa-tag fa-sm"></i> learning</a>   </p> </header> <article class="post-content"> <h2 id="introduction">Introduction</h2> <p>During one of my biweekly research meetings, my group reviewed <em><a href="https://arxiv.org/abs/1806.09471" target="_blank" rel="noopener noreferrer">Does data interpolation contradict statistical optimality?</a></em> by Mikhail Belkin, Alexander Rakhlin, and Alexandre B, Tsybakov</p> <p>The aim of this paper was to show that interpolating training data can still lead to optimal results in nonparametric regression and prediction with square loss. Since the double descent phenomenon exhibits itself when the model capacity surpasses the “interpolation threshold”, I thought that reproducing the results from this paper would help me understand how a model interpolates data.</p> <p>[1]</p> <p><img src="/assets/img/media/double_descent.png" alt="" style="height: 75%; width: 75%; object-fit: contain">)</p> <h2 id="math-and-code">Math and Code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">numpy.linalg</span> <span class="k">as</span> <span class="n">lin</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
</code></pre></div></div> <p>This paper takes a look at interpolation using the <strong>Nadaraya-Watson Estimator</strong>.</p> <p>Let \((X, Y)\) be a random pair on \(\mathbb{R}^{d} \times \mathbb{R}\) with distribution \(P_{XY}\), and let \(\mathbb{E}[Y \vert X = x]\) be the regression function.</p> <p>Given a sample \((X_{1}, Y_{1}),...,(X_{n}, Y_{n})\) drawn independently from \(P_{XY}\), we can approximate $f(x)$ using the Nadaraya-Watson Estimator where \(K: \mathbb{R}^{d} \rightarrow \mathbb{R}\) is a kernel function and \(h &gt; 0\) is a bandwidth</p> <p><img src="/assets/img/media/nw_estimator.png" alt="" style="height: 50%; width: 50%; object-fit: contain"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">):</span>
    <span class="n">cols</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>

        <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nc">K</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">h</span><span class="p">)))</span>


    <span class="n">Kx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">cols</span><span class="p">))</span>

    <span class="n">row_sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Kx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">Kx</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">result</span><span class="p">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p><strong>Note:</strong> Since we are dealing with singular kernels that approach infinity when their argument tends to zero, we will have to use a modified version</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>

        <span class="n">condition</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">for</span> <span class="n">boolean</span> <span class="ow">in</span> <span class="p">[</span><span class="n">k</span><span class="o">==</span><span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]]:</span>
            <span class="k">if</span> <span class="n">boolean</span><span class="p">:</span>
                <span class="n">condition</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">condition</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
            <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nc">K</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">h</span><span class="p">,</span> <span class="n">a</span><span class="p">)))</span>


    <span class="n">Kx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">cols</span><span class="p">))</span>

    <span class="n">row_sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Kx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">Kx</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">result</span><span class="p">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>The two singular kernels we will be focusing on are:</p> <p><img src="/assets/img/media/sing_kernel_1.png" alt="" style="height: 50%; width: 50%; object-fit: contain"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">sing_kernel_1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="n">a</span>
</code></pre></div></div> <p>and</p> <p><img src="/assets/img/media/sing_kernel_2.png" alt="" style="height: 50%; width: 50%; object-fit: contain"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">sing_kernel_2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="n">a</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</code></pre></div></div> <p>The data generating functions we will look at are</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">actual_regression_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>and</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">binary_classification</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([])</span>

    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">4</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">outs</span>
</code></pre></div></div> <h3 id="some-notes">Some notes:</h3> <ul> <li>Because of input size mismatches, I used <code class="language-python highlighter-rouge"><span class="nf">abs</span><span class="p">()</span></code> instead of <code class="language-python highlighter-rouge"><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span></code> since I pass in the elements as arrays, but they are really (x, y) which are both just 1 dimensional real numbers</li> <li>The indicator function on <code class="language-python highlighter-rouge"><span class="n">sing_kernel_1</span></code> gave me errors when I tried implementing it, so i removed it. The kernel’s singularity as the argument goes to infinity is still present.</li> <li>The data from the sinusoidal function (p.4 of the paper) does not seem to be randomly sampled, so I did not randomly sample it either</li> </ul> <h2 id="results">Results</h2> <h3 id="sine-curve-with-singular_kernel_1">Sine curve with <code class="language-python highlighter-rouge"><span class="n">singular_kernel_1</span></code>:</h3> <p><img src="/assets/img/media/sine_nw_singular.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"></p> <p>The estimator fits the curve fairly well for values of <em>a</em> &gt; .8. For some reason, the bandwidth, <em>h</em>, doesn’t do anything to this specific example. Because the best value of <em>h</em> in the paper was .4, I chose to keep h constant.</p> <h3 id="sine-curve-with-standard-gaussian-kernel">Sine curve with standard Gaussian kernel:</h3> <p><img src="/assets/img/media/non_sing_sine.png" alt="" style="height: 60%; width: 60%; object-fit: contain"></p> <p>There are no animations for different parameters because the only tunable parameter is the bandwidth, <em>h</em>, which was held constant at .4</p> <h4 id="code-from-notebook">Code (from notebook)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#np.random.seed(100)
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">#epsilon = np.random.normal(loc = 0, scale = 2, size = n)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1">#np.random.normal(loc = 0, scale = 3, size=n)
</span><span class="n">Y</span> <span class="o">=</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># Singular Kernel
</span><span class="n">h</span><span class="o">=</span><span class="p">.</span><span class="mi">4</span>
<span class="n">a</span><span class="o">=</span><span class="mf">1.5</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">x_axis</span><span class="p">),</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">sing_kernel_1</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">True Regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># Non-singular Kernel
</span>
<span class="n">h</span><span class="o">=</span><span class="n">n</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">x_axis</span><span class="p">),</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">True Regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="binary-data-with-singular_kernel_2">Binary Data with <code class="language-python highlighter-rouge"><span class="n">singular_kernel_2</span></code>:</h3> <p><img src="/assets/img/media/binary_sing.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"></p> <p>In this animation, I sweep through several values for <em>h</em> with <em>a</em> constant. Then I hold <em>h</em> constant and sweep through several values of <em>a</em></p> <h3 id="binary-data-with-standard-gaussian-kernel">Binary data with standard Gaussian kernel</h3> <p><img src="/assets/img/media/non_sing_binary.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"></p> <p>The only tunable parameter here is <em>h</em></p> <h4 id="code-from-notebook-1">Code (from notebook)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#np.random.seed(100)
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">#epsilon = np.random.normal(loc = 0, scale = 2, size = n)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="nf">binary_distribution</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">h</span><span class="o">=</span><span class="n">n</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)))</span>
<span class="n">a</span><span class="o">=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)),</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">sing_kernel_2</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">a = {:.2f}; h = {:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Boundary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">h</span> <span class="o">=</span><span class="p">.</span><span class="mi">4</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)),</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">h = {:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Boundary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>After reproducing these results, I emailed Mikhail Belkin to get his thoughts on the connection between this paper and a previous paper he wrote on the double descent phenomenon. His reply was very similar to what my group thought the connection may be: <strong>interpolation is consistent with the current practice of deep learning.</strong> The most important thing to realize is that there is still not a proven, complete connection between modern machine learning methods and the model shown in this post.</p> <p>References:</p> <ul> <li>Does data interpolation contradict statistical optimality? - <a href="https://arxiv.org/abs/1806.09471" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/1806.09471</a> </li> <li>[1] - <a href="https://www.cs.ubc.ca/labs/lci/mlrg/slides/dl_generalization.pdf" target="_blank" rel="noopener noreferrer">https://www.cs.ubc.ca/labs/lci/mlrg/slides/dl_generalization.pdf</a> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 John Abascal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YBQJC2ZQY4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YBQJC2ZQY4");</script> </body> </html>