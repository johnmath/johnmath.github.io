<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content=""/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Double Descent Testbed | John Abascal</title> <meta name="author" content="John Abascal"/> <meta name="description" content=""/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://johnmath.github.io/blog/2022/double-descent-testbed/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">John </span>Abascal</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">research</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Double Descent Testbed</h1> <p class="post-meta">February 14, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/machine"> <i class="fas fa-tag fa-sm"></i> machine</a>   <a href="/blog/category/learning"> <i class="fas fa-tag fa-sm"></i> learning</a>   </p> </header> <article class="post-content"> <ol> <li><a href="#the-double-descent-phenomenon">The Double Descent Phenomenon </a></li> <li><a href="#why-we-created-the-testbed">Why We Created the Testbed</a></li> <li> <a href="#project-architecture">Project Architecture</a> <ol> <li><a href="#data">Data</a></li> <li> <a href="#models">Models</a> <ol> <li><a href="#multilayer-perceptron">Multilayer Perceptron</a></li> <li><a href="#random-forest">Random Forest</a></li> </ol> </li> <li> <a href="#utils">Utils</a> <ol> <li><a href="#parameter-count-generation-algorithm">Parameter Count Generation Algorithm</a></li> <li><a href="#double-descent-training-loop">Double Descent Training Loop</a></li> </ol> </li> </ol> </li> </ol> <h2 id="the-double-descent-phenomenon">The Double Descent Phenomenon</h2> <p>Modern machine learning models, such as deep neural networks, have given us the ability to observe how models with immense hypothesis classes (i.e. the set of functions that are learnable by the mode) are able to perform on complex, high dimensional data sets. Based on what we know about the bias-variance tradeoff, if we keep on increasing the complexity of our model (keeping the dataset fixed), we will reach a model capacity that causes the model to overfit and perform worse on out-of-sample data. However, today’s neural networks seem to contradict this. Even with millions of parameters, they seemingly perform better than smaller models on most tasks. This has led to the conventional wisdom in the deep learning community that “larger models are better”. The ability of massive machine learning models to consistently outperform smaller models has led researchers to believe that our current model of the bias-variance tradeoff may be incomplete.</p> <p>Recent empirical evidence suggests that there exists more than one “training regime” in today’s machine learning practices. This second regime is proposed to exhibit itself when our hypothesis class is so large that our model is well past simply interpolating the data (i.e. when our model’s empirical loss, \(\mathcal{L}_{S}(h) = 0\)). Typically, we would consider a model with \(0\) training loss to be overfitting the data, but this may not be the case. <a href="https://arxiv.org/pdf/1806.09471.pdf" target="_blank" rel="noopener noreferrer">A 2018 paper by Belkin et al.</a> shows that interpolating the training data can achieve good generalization in nonparametric regression problems.</p> <table> <thead> <tr> <th>Binary Classification</th> <th>Regression</th> </tr> </thead> <tbody> <tr> <td><img src="/assets/img/media/binary_sing.gif" alt="sing_binary" width="400"></td> <td><img src="/assets/img/media/sine_nw_singular.gif" alt="sing_sine" width="400"></td> </tr> </tbody> </table> <p><strong>Fig 1. Our Reproduction of the Interpolating Nadaraya-Watson Estimator for Classification and Regression</strong></p> <p>Since this paper was published, <a href="https://openai.com/blog/deep-double-descent/" target="_blank" rel="noopener noreferrer">this idea has been extended to deep learning models</a>, and the results have matched Belkin’s results with smaller-scale nonparametric regression.</p> <p><img align="center" width="800" height="auto" src="/assets/img/media/double-descent.png" alt="OpenAI-DD"></p> <p><strong>Fig 2. The Double Descent Curve</strong></p> <p>Because of the two U-shaped curves in the plot of test risk vs model capacity, this phenomenon where the model achieves better generalization as the capacity of the hypothesis class, \(\mathcal{H}\), increases is called <strong>the double descent phenomenon</strong>.</p> <h2 id="why-we-created-the-testbed">Why We Created the Testbed</h2> <p>Since double descent is a new phenomenon, there is no central hub where researchers can compare and contrast each other’s results. Researchers Ishaan Gulrajani and David Lopez-Paz from Facebook noticed there was a similar problem in the field of <em>domain generalization</em>. This led them to create <a href="https://github.com/facebookresearch/DomainBed" target="_blank" rel="noopener noreferrer">DomainBed</a>, a testbed for domain generalization. They implemented seven multi-domain datasets, nine baseline algorithms, and three model selection criteria and are allowing domain generalization researchers to contribute to the testbed.</p> <p>The goal of this project was to create a similar platform for double descent researchers. DomainBed takes proven algorithms and allows researchers to consistently reproduce results and directly compare algorithms. Given that double descent has been shown to appear in several models with different datasets, creating a testbed that includes these models and datasets would fix a major issue in this research field. Understanding the double descent phenomenon can potentially lead to more robust and accurate machine learning algorithms at no “extra cost”, therefore it is imperative that there is some kind of standardized way to research it.</p> <h2 id="project-architecture">Project Architecture</h2> <p><img align="center" src="/assets/img/media/honors_work_module.png" alt="DD_TestBed" width="75%"></p> <p><strong>Fig 3. Architecture of the Double Descent Testbed</strong></p> <p>The testbed has been designed in an object oriented way. This allows users to simply import models from the module, run two or three commands, and have complex experiments running without any boilerplate code. This platform is designed for scientists, though, so users will be given access to the source code and all of the included utilities. This will aid in allowing unique experiments to be conducted using the platform as users can choose the level of abstraction or granularity that they are comfortable with without having to write most of the code themselves.</p> <p>The project consists of four sub-modules: <strong>models</strong>, <strong>data</strong>, <strong>utils</strong>, and <strong>plots</strong>. The <strong>models</strong> sub-module contains abstracted versions of models that have exhibited double descent. At this time, <strong>models</strong> consists of a fully connected neural network model and a random forest classifier. The <strong>data</strong> sub-module contains abstracted versions of datasets that have been used in double descent experiments. The models are written in PyTorch or scikit-learn, so the <strong>data</strong> sub-module is divided into two parts, TorchData and SKLearnData. This allows for more compatibility when a user wants to add a model from either library. The <strong>utils</strong> sub-module contains any tools that are needed to train models, process data, etc. The main feature of <strong>utils</strong> is a parameter count generation algorithm that will be discussed later on in the blog post. Lastly, the <strong>plots</strong> sub-module contains a class with utilities that are specific to plotting, written using Matplotlib. These utilities can be used with the data that is returned by the models after training.</p> <h3 id="data">Data</h3> <p>The <strong>data</strong> module has two versions of the MNIST dataset: a PyTorch implementation and a scikit-learn implementation. The PyTorch implementation contains two PyTorch dataloaders along with exposed parameters such as batch sizes and number of training samples. The PyTorch dataloaders are iterable objects that contain a dataset object within them. By using a dataloader, we can apply transformations to the data and shuffle the dataset prior to training our model. The scikit-learn implementation of MNIST simply returns an \(N \times 784\) matrix where \(N\) is the number of training samples and each row is a 784-dimensional vector that encodes a 28 x 28 image. Both of these implementations download and save the dataset to a local repository where it can be reused without having to wait for the data to be downloaded a second time.</p> <p><img align="center" src="/assets/img/media/mnist.jpeg" alt="MNIST" width="40%"></p> <p><strong>Fig 4. A Sample of Images from the MNIST Dataset</strong></p> <h3 id="models">Models</h3> <h4 id="multilayer-perceptron">Multilayer Perceptron</h4> <p>The multilayer perceptron is made up of 3 layers, an input layer, a hidden layer, and an output layer (This type of model is sometimes referred to as a two-layer neural network). All three have variable size depending on the dataset that is being trained on. The input layer has \(d = n \cdot m\) units where \(n\) and \(m\) are the dimensions of the input data, as the images are flattened before being passed through the network. The hidden layer has \(h_{i}\) units, where \(h_{i}\) is computed using a desired number of total parameters and the formula for total parameters in Figure 3. The output layer has \(K\) units where \(K\) is the number of output classes. This model also has ReLU activation functions on both the input layer and hidden layer.</p> <p><img align="center" src="/assets/img/media/param_counts_eq.png" alt="audit_params" width="400"></p> <p><strong>Fig 5. Equation to Calculate Number of Parameters in a Two-Layer Neural Network</strong></p> <p><img align="center" src="/assets/img/media/MLP.png" alt="MLP_Arch" width="400"></p> <p><strong>Fig 6. Visualization of our Two-Layer Neural Network</strong></p> <p>One main feature of the multilayer perceptron wrapper is its built-in <a href="https://www.tensorflow.org/tensorboard/get_started" target="_blank" rel="noopener noreferrer">TensorBoard</a> functionality. TensorBoard is a visualization dashboard for machine learning experiments. It runs in a web server and reads from a log directory that is produced by the neural network training loop in the double descent testbed. Throughout the training process, we log all training and testing losses for each individual model, as well as the final losses of each model. On this dashboard, we also expose the architecture of the current model that is being experimented on (i.e. its computational graph) and a sample of the dataset that is being used to train the model.</p> <p><img align="center" src="/assets/img/media/TensorBoard-Sample2.png" alt="Tensorboard" width="75%"></p> <p><strong>Fig 7. A Screenshot of Tensorboard (Test Loss vs. Model Capacity in # parameters)</strong></p> <h4 id="random-forest">Random Forest</h4> <p>The random forest classifier wrapper follows suit by exposing certain features and parameters, such as the maximum number of leaf nodes, the number of trees, and the criterion, to the user when they create an instance of it. Unlike the multilayer perceptron wrapper, which is implemented in PyTorch, the random forest wrapper is implemented in scikit-learn. This means that there is no GPU support, and we cannot take advantage of built-in PyTorch dataloaders. This is not an issue that needs immediate attention, though, as these classifiers only took about half a minute to train in our experiments. Unifying all models under the PyTorch umbrella will be a main area of future work on this testbed.</p> <h3 id="utils">Utils</h3> <h4 id="parameter-count-generation-algorithm">Parameter Count Generation Algorithm</h4> <p>When designing the neural network wrapper, we noticed that each of the models took a while to train. Even though the individual models are quite small, when training several of them sequentially for a large number of epochs, single experiments could take several days. To help reduce the time that it takes to run experiments, we developed a parameter count generation algorithm to intelligently choose the next model to train using the number of parameters in the previous model and the final test loss that the model produced. This allowed us to avoid iterating through model capacities (i.e numbers of parameters in the model) at a fixed, constant value. The algorithm was designed to have the highest resolution around the area where models interpolate, or overfit, the data, as this is where the double descent curve is supposed to exhibit itself. We do this by assuming that the double descent curve should look roughly like a \(3^{rd}\) degree polynomial, fitting a \(3^{rd}\) degree polynomial to the model capacity vs. test loss graph, and examining the first derivative of the polynomial that was fit to the data. Algorithm 1 shows the pseudocode for this parameter count generation algorithm.</p> <p>The algorithm takes a list of previous parameter counts, a list of previous test losses, a flag to determine if the interpolation threshold has been reached, and a tuning parameter \(\alpha\) as input</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">Parameter_Count_Generation</span><span class="p">(</span><span class="n">param_counts</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    
    <span class="n">current_count</span> <span class="o">=</span> <span class="n">param_counts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Take last element of param_counts
</span>    
    <span class="c1"># Weight more recent parameter counts more heavily
</span>    <span class="n">weight_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="p">...</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># where n = len(param_counts)
</span>    
    <span class="n">poly</span> <span class="o">=</span> <span class="nf">fit_polynomial</span><span class="p">(</span><span class="n">param_counts</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># Examine the first derivative of the polynomial 
</span>    <span class="n">dy</span> <span class="o">=</span> <span class="nf">dy_poly</span><span class="p">(</span><span class="n">current_count</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">dy</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sgn</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sgn</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="bp">True</span>
        
    <span class="n">next_count</span> <span class="o">=</span> <span class="n">sgn</span><span class="o">*</span><span class="nf">max</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">dy</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    
    <span class="k">if</span> <span class="n">sgn</span> <span class="ow">and</span> <span class="n">past_interpolation_threshold</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span> <span class="o">+</span> <span class="n">current_count</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span>
    <span class="k">else</span>
        <span class="k">return</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span> <span class="o">+</span> <span class="n">current_count</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span>
</code></pre></div></div> <p><img align="center" width="800" height="auto" src="/assets/img/media/dd_2.gif" alt="param_gen"></p> <p><strong>Fig 8. Our Parameter Counts Generation Algorithm Running on a Synthetic Double Descent Curve</strong></p> <h4 id="double-descent-training-loop">Double Descent Training Loop</h4> <p>Each of the models in the testbed has an associated <em>double_descent</em> method that performs the same training procedure for a model over several different capacities. In general, the <em>double_descent</em> method loops over a chosen list of parameter counts for a model (or generates them in an online fashion using the parameter count generation algorithm) and trains the model to any completion criterion that the user has chosen. At the end of each training procedure, final train losses, final test losses, and parameter counts for models of different capacities are aggregated and output to a dictionary of arrays that can be used for visualization or analysis. In the case of the <strong>MultilayerPerceptron</strong> class, this method contains TensorBoard functionality. To speed up convergence on the multilayer perceptron model, we can toggle a <code class="language-python highlighter-rouge"><span class="n">reuse_weights</span></code> flag to use the weights from the previous model as an initialization for the next model.</p> <p>If there is a pre-spectified list of parameters, the double descent training loop is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">param_counts</span><span class="p">)):</span>
    
    <span class="n">current_index</span> <span class="o">=</span> <span class="n">i</span>
    
    <span class="n">current_parameter_count</span> <span class="o">=</span> <span class="n">parameter_counts</span><span class="p">[</span><span class="n">current_index</span><span class="p">]</span>
    
    <span class="c1"># Creates new model with specified number of parameters
</span>    <span class="nf">reinitialize_model</span><span class="p">(</span><span class="n">current_parameter_count</span><span class="p">)</span>
    
    <span class="n">losses</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">()</span>

    <span class="c1"># Log losses into TensorBoard or arrays
</span></code></pre></div></div> <p>If the user wants to generate the parameter counts, after a single iteration of training (above), the double descent training loop is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code>
<span class="k">if</span> <span class="n">generate_parameters_enabled</span><span class="p">:</span>
    
    <span class="n">steps_past_dd</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">while</span> <span class="n">steps_past_dd</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        
        <span class="n">next_count</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="nc">Parameter_Count_Generation</span><span class="p">(</span><span class="n">parameter_counts</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">)</span>
        
        <span class="n">parameter_counts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span>
        
        <span class="n">current_parameter_counts</span> <span class="o">=</span> <span class="n">parameter_counts</span><span class="p">[</span><span class="n">current_index</span><span class="p">]</span>
        
        <span class="nf">reinitialize_model</span><span class="p">(</span><span class="n">current_parameter_count</span><span class="p">)</span>
    
        <span class="n">losses</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">()</span>
        
        <span class="c1"># Log losses into TensorBoard or arrays
</span>        
        <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">past_interpolation_threshold</span><span class="p">:</span>
            <span class="n">steps_past_dd</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 John Abascal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YBQJC2ZQY4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YBQJC2ZQY4");</script> </body> </html>