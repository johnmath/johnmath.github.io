<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://johnmath.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://johnmath.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-27T02:00:16+00:00</updated><id>https://johnmath.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Double Descent Testbed</title><link href="https://johnmath.github.io/blog/2022/double-descent-testbed/" rel="alternate" type="text/html" title="Double Descent Testbed"/><published>2022-02-14T00:35:00+00:00</published><updated>2022-02-14T00:35:00+00:00</updated><id>https://johnmath.github.io/blog/2022/double-descent-testbed</id><content type="html" xml:base="https://johnmath.github.io/blog/2022/double-descent-testbed/"><![CDATA[<ol> <li><a href="#the-double-descent-phenomenon">The Double Descent Phenomenon </a></li> <li><a href="#why-we-created-the-testbed">Why We Created the Testbed</a></li> <li><a href="#project-architecture">Project Architecture</a> <ol> <li><a href="#data">Data</a></li> <li><a href="#models">Models</a> <ol> <li><a href="#multilayer-perceptron">Multilayer Perceptron</a></li> <li><a href="#random-forest">Random Forest</a></li> </ol> </li> <li><a href="#utils">Utils</a> <ol> <li><a href="#parameter-count-generation-algorithm">Parameter Count Generation Algorithm</a></li> <li><a href="#double-descent-training-loop">Double Descent Training Loop</a></li> </ol> </li> </ol> </li> </ol> <h2 id="the-double-descent-phenomenon">The Double Descent Phenomenon</h2> <p>Modern machine learning models, such as deep neural networks, have given us the ability to observe how models with immense hypothesis classes (i.e. the set of functions that are learnable by the mode) are able to perform on complex, high dimensional data sets. Based on what we know about the bias-variance tradeoff, if we keep on increasing the complexity of our model (keeping the dataset fixed), we will reach a model capacity that causes the model to overfit and perform worse on out-of-sample data. However, today’s neural networks seem to contradict this. Even with millions of parameters, they seemingly perform better than smaller models on most tasks. This has led to the conventional wisdom in the deep learning community that “larger models are better”. The ability of massive machine learning models to consistently outperform smaller models has led researchers to believe that our current model of the bias-variance tradeoff may be incomplete.</p> <p>Recent empirical evidence suggests that there exists more than one “training regime” in today’s machine learning practices. This second regime is proposed to exhibit itself when our hypothesis class is so large that our model is well past simply interpolating the data (i.e. when our model’s empirical loss, \(\mathcal{L}_{S}(h) = 0\)). Typically, we would consider a model with \(0\) training loss to be overfitting the data, but this may not be the case. <a href="https://arxiv.org/pdf/1806.09471.pdf">A 2018 paper by Belkin et al.</a> shows that interpolating the training data can achieve good generalization in nonparametric regression problems.</p> <table> <thead> <tr> <th>Binary Classification</th> <th>Regression</th> </tr> </thead> <tbody> <tr> <td><img src="/assets/img/media/binary_sing.gif" alt="sing_binary" width="400"/></td> <td><img src="/assets/img/media/sine_nw_singular.gif" alt="sing_sine" width="400"/></td> </tr> </tbody> </table> <p><strong>Fig 1. Our Reproduction of the Interpolating Nadaraya-Watson Estimator for Classification and Regression</strong></p> <p>Since this paper was published, <a href="https://openai.com/blog/deep-double-descent/">this idea has been extended to deep learning models</a>, and the results have matched Belkin’s results with smaller-scale nonparametric regression.</p> <p><img align="center" width="800" height="auto" src="/assets/img/media/double-descent.png" alt="OpenAI-DD"/></p> <p><strong>Fig 2. The Double Descent Curve</strong></p> <p>Because of the two U-shaped curves in the plot of test risk vs model capacity, this phenomenon where the model achieves better generalization as the capacity of the hypothesis class, \(\mathcal{H}\), increases is called <strong>the double descent phenomenon</strong>.</p> <h2 id="why-we-created-the-testbed">Why We Created the Testbed</h2> <p>Since double descent is a new phenomenon, there is no central hub where researchers can compare and contrast each other’s results. Researchers Ishaan Gulrajani and David Lopez-Paz from Facebook noticed there was a similar problem in the field of <em>domain generalization</em>. This led them to create <a href="https://github.com/facebookresearch/DomainBed">DomainBed</a>, a testbed for domain generalization. They implemented seven multi-domain datasets, nine baseline algorithms, and three model selection criteria and are allowing domain generalization researchers to contribute to the testbed.</p> <p>The goal of this project was to create a similar platform for double descent researchers. DomainBed takes proven algorithms and allows researchers to consistently reproduce results and directly compare algorithms. Given that double descent has been shown to appear in several models with different datasets, creating a testbed that includes these models and datasets would fix a major issue in this research field. Understanding the double descent phenomenon can potentially lead to more robust and accurate machine learning algorithms at no “extra cost”, therefore it is imperative that there is some kind of standardized way to research it.</p> <h2 id="project-architecture">Project Architecture</h2> <p><img align="center" src="/assets/img/media/honors_work_module.png" alt="DD_TestBed" width="75%"/></p> <p><strong>Fig 3. Architecture of the Double Descent Testbed</strong></p> <p>The testbed has been designed in an object oriented way. This allows users to simply import models from the module, run two or three commands, and have complex experiments running without any boilerplate code. This platform is designed for scientists, though, so users will be given access to the source code and all of the included utilities. This will aid in allowing unique experiments to be conducted using the platform as users can choose the level of abstraction or granularity that they are comfortable with without having to write most of the code themselves.</p> <p>The project consists of four sub-modules: <strong>models</strong>, <strong>data</strong>, <strong>utils</strong>, and <strong>plots</strong>. The <strong>models</strong> sub-module contains abstracted versions of models that have exhibited double descent. At this time, <strong>models</strong> consists of a fully connected neural network model and a random forest classifier. The <strong>data</strong> sub-module contains abstracted versions of datasets that have been used in double descent experiments. The models are written in PyTorch or scikit-learn, so the <strong>data</strong> sub-module is divided into two parts, TorchData and SKLearnData. This allows for more compatibility when a user wants to add a model from either library. The <strong>utils</strong> sub-module contains any tools that are needed to train models, process data, etc. The main feature of <strong>utils</strong> is a parameter count generation algorithm that will be discussed later on in the blog post. Lastly, the <strong>plots</strong> sub-module contains a class with utilities that are specific to plotting, written using Matplotlib. These utilities can be used with the data that is returned by the models after training.</p> <h3 id="data">Data</h3> <p>The <strong>data</strong> module has two versions of the MNIST dataset: a PyTorch implementation and a scikit-learn implementation. The PyTorch implementation contains two PyTorch dataloaders along with exposed parameters such as batch sizes and number of training samples. The PyTorch dataloaders are iterable objects that contain a dataset object within them. By using a dataloader, we can apply transformations to the data and shuffle the dataset prior to training our model. The scikit-learn implementation of MNIST simply returns an \(N \times 784\) matrix where \(N\) is the number of training samples and each row is a 784-dimensional vector that encodes a 28 x 28 image. Both of these implementations download and save the dataset to a local repository where it can be reused without having to wait for the data to be downloaded a second time.</p> <p><img align="center" src="/assets/img/media/mnist.jpeg" alt="MNIST" width="40%"/></p> <p><strong>Fig 4. A Sample of Images from the MNIST Dataset</strong></p> <h3 id="models">Models</h3> <h4 id="multilayer-perceptron">Multilayer Perceptron</h4> <p>The multilayer perceptron is made up of 3 layers, an input layer, a hidden layer, and an output layer (This type of model is sometimes referred to as a two-layer neural network). All three have variable size depending on the dataset that is being trained on. The input layer has \(d = n \cdot m\) units where \(n\) and \(m\) are the dimensions of the input data, as the images are flattened before being passed through the network. The hidden layer has \(h_{i}\) units, where \(h_{i}\) is computed using a desired number of total parameters and the formula for total parameters in Figure 3. The output layer has \(K\) units where \(K\) is the number of output classes. This model also has ReLU activation functions on both the input layer and hidden layer.</p> <p><img align="center" src="/assets/img/media/param_counts_eq.png" alt="audit_params" width="400"/></p> <p><strong>Fig 5. Equation to Calculate Number of Parameters in a Two-Layer Neural Network</strong></p> <p><img align="center" src="/assets/img/media/MLP.png" alt="MLP_Arch" width="400"/></p> <p><strong>Fig 6. Visualization of our Two-Layer Neural Network</strong></p> <p>One main feature of the multilayer perceptron wrapper is its built-in <a href="https://www.tensorflow.org/tensorboard/get_started">TensorBoard</a> functionality. TensorBoard is a visualization dashboard for machine learning experiments. It runs in a web server and reads from a log directory that is produced by the neural network training loop in the double descent testbed. Throughout the training process, we log all training and testing losses for each individual model, as well as the final losses of each model. On this dashboard, we also expose the architecture of the current model that is being experimented on (i.e. its computational graph) and a sample of the dataset that is being used to train the model.</p> <p><img align="center" src="/assets/img/media/TensorBoard-Sample2.png" alt="Tensorboard" width="75%"/></p> <p><strong>Fig 7. A Screenshot of Tensorboard (Test Loss vs. Model Capacity in # parameters)</strong></p> <h4 id="random-forest">Random Forest</h4> <p>The random forest classifier wrapper follows suit by exposing certain features and parameters, such as the maximum number of leaf nodes, the number of trees, and the criterion, to the user when they create an instance of it. Unlike the multilayer perceptron wrapper, which is implemented in PyTorch, the random forest wrapper is implemented in scikit-learn. This means that there is no GPU support, and we cannot take advantage of built-in PyTorch dataloaders. This is not an issue that needs immediate attention, though, as these classifiers only took about half a minute to train in our experiments. Unifying all models under the PyTorch umbrella will be a main area of future work on this testbed.</p> <h3 id="utils">Utils</h3> <h4 id="parameter-count-generation-algorithm">Parameter Count Generation Algorithm</h4> <p>When designing the neural network wrapper, we noticed that each of the models took a while to train. Even though the individual models are quite small, when training several of them sequentially for a large number of epochs, single experiments could take several days. To help reduce the time that it takes to run experiments, we developed a parameter count generation algorithm to intelligently choose the next model to train using the number of parameters in the previous model and the final test loss that the model produced. This allowed us to avoid iterating through model capacities (i.e numbers of parameters in the model) at a fixed, constant value. The algorithm was designed to have the highest resolution around the area where models interpolate, or overfit, the data, as this is where the double descent curve is supposed to exhibit itself. We do this by assuming that the double descent curve should look roughly like a \(3^{rd}\) degree polynomial, fitting a \(3^{rd}\) degree polynomial to the model capacity vs. test loss graph, and examining the first derivative of the polynomial that was fit to the data. Algorithm 1 shows the pseudocode for this parameter count generation algorithm.</p> <p>The algorithm takes a list of previous parameter counts, a list of previous test losses, a flag to determine if the interpolation threshold has been reached, and a tuning parameter \(\alpha\) as input</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">Parameter_Count_Generation</span><span class="p">(</span><span class="n">param_counts</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    
    <span class="n">current_count</span> <span class="o">=</span> <span class="n">param_counts</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Take last element of param_counts
</span>    
    <span class="c1"># Weight more recent parameter counts more heavily
</span>    <span class="n">weight_vector</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="p">...</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># where n = len(param_counts)
</span>    
    <span class="n">poly</span> <span class="o">=</span> <span class="nf">fit_polynomial</span><span class="p">(</span><span class="n">param_counts</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    
    <span class="c1"># Examine the first derivative of the polynomial 
</span>    <span class="n">dy</span> <span class="o">=</span> <span class="nf">dy_poly</span><span class="p">(</span><span class="n">current_count</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">dy</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sgn</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sgn</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="bp">True</span>
        
    <span class="n">next_count</span> <span class="o">=</span> <span class="n">sgn</span><span class="o">*</span><span class="nf">max</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">dy</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    
    <span class="k">if</span> <span class="n">sgn</span> <span class="ow">and</span> <span class="n">past_interpolation_threshold</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span> <span class="o">+</span> <span class="n">current_count</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span>
    <span class="k">else</span>
        <span class="k">return</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span> <span class="o">+</span> <span class="n">current_count</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span>
</code></pre></div></div> <p><img align="center" width="800" height="auto" src="/assets/img/media/dd_2.gif" alt="param_gen"/></p> <p><strong>Fig 8. Our Parameter Counts Generation Algorithm Running on a Synthetic Double Descent Curve</strong></p> <h4 id="double-descent-training-loop">Double Descent Training Loop</h4> <p>Each of the models in the testbed has an associated <em>double_descent</em> method that performs the same training procedure for a model over several different capacities. In general, the <em>double_descent</em> method loops over a chosen list of parameter counts for a model (or generates them in an online fashion using the parameter count generation algorithm) and trains the model to any completion criterion that the user has chosen. At the end of each training procedure, final train losses, final test losses, and parameter counts for models of different capacities are aggregated and output to a dictionary of arrays that can be used for visualization or analysis. In the case of the <strong>MultilayerPerceptron</strong> class, this method contains TensorBoard functionality. To speed up convergence on the multilayer perceptron model, we can toggle a <code class="language-python highlighter-rouge"><span class="n">reuse_weights</span></code> flag to use the weights from the previous model as an initialization for the next model.</p> <p>If there is a pre-spectified list of parameters, the double descent training loop is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">param_counts</span><span class="p">)):</span>
    
    <span class="n">current_index</span> <span class="o">=</span> <span class="n">i</span>
    
    <span class="n">current_parameter_count</span> <span class="o">=</span> <span class="n">parameter_counts</span><span class="p">[</span><span class="n">current_index</span><span class="p">]</span>
    
    <span class="c1"># Creates new model with specified number of parameters
</span>    <span class="nf">reinitialize_model</span><span class="p">(</span><span class="n">current_parameter_count</span><span class="p">)</span>
    
    <span class="n">losses</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">()</span>

    <span class="c1"># Log losses into TensorBoard or arrays
</span></code></pre></div></div> <p>If the user wants to generate the parameter counts, after a single iteration of training (above), the double descent training loop is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code>
<span class="k">if</span> <span class="n">generate_parameters_enabled</span><span class="p">:</span>
    
    <span class="n">steps_past_dd</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="bp">False</span>
    
    <span class="k">while</span> <span class="n">steps_past_dd</span> <span class="o">&lt;</span> <span class="mi">4</span><span class="p">:</span>
        
        <span class="n">next_count</span><span class="p">,</span> <span class="n">past_interpolation_threshold</span> <span class="o">=</span> <span class="nc">Parameter_Count_Generation</span><span class="p">(</span><span class="n">parameter_counts</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">)</span>
        
        <span class="n">parameter_counts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">next_count</span><span class="p">)</span>
        
        <span class="n">current_parameter_counts</span> <span class="o">=</span> <span class="n">parameter_counts</span><span class="p">[</span><span class="n">current_index</span><span class="p">]</span>
        
        <span class="nf">reinitialize_model</span><span class="p">(</span><span class="n">current_parameter_count</span><span class="p">)</span>
    
        <span class="n">losses</span> <span class="o">=</span> <span class="nf">train_model</span><span class="p">()</span>
        
        <span class="c1"># Log losses into TensorBoard or arrays
</span>        
        <span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">if</span> <span class="n">past_interpolation_threshold</span><span class="p">:</span>
            <span class="n">steps_past_dd</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[The Double Descent Phenomenon Why We Created the Testbed Project Architecture Data Models Multilayer Perceptron Random Forest Utils Parameter Count Generation Algorithm Double Descent Training Loop]]></summary></entry><entry><title type="html">Binet’s Formula and Extensions of the Fibonacci Numbers</title><link href="https://johnmath.github.io/blog/2020/binets-formula/" rel="alternate" type="text/html" title="Binet’s Formula and Extensions of the Fibonacci Numbers"/><published>2020-12-02T00:35:00+00:00</published><updated>2020-12-02T00:35:00+00:00</updated><id>https://johnmath.github.io/blog/2020/binets-formula</id><content type="html" xml:base="https://johnmath.github.io/blog/2020/binets-formula/"><![CDATA[<p><strong>Binet’s Formula:</strong> \(F_{n} = \frac{1}{\sqrt{5}} \left(\left(\frac{1 + \sqrt{5}}{2}\right) ^{n} - \left(\frac{1 - \sqrt{5}}{2}\right)^{n}\right)\)</p> <p>where \(F_{n}\) is the \(n^{th}\) Fibonacci number (the recursive definition for the Fibonacci numbers is \(F_{n} = F_{n-1} + F_{n-2}\) where \(F_{1} = 1\) and \(F_{2} = 1\). We can also include \(F_{0} = 0\))</p> <p>Typically, Binet’s formula over \(\mathbb{N}\) gives us \(F_{1} = 1, F_{2} = 1, F_{3} = 2\) …, but what happens when we use Binet’s formula to find the “\(0.5^{th}\) Fibonacci number” or the “\(\pi^{th}\) Fibonacci number” (if they even exist)? Well, if we try to find \(F_{\pi}\), what we end up with is roughly \(2.11702 + 0.04244i\). We end up with complex numbers because trying to find \(F_{n}\) where \(n \not\in \mathbb{N}\) leads to complex outputs. So, let’s take a look at the outputs of Binet’s formula over some continuous, real domain (e.g. \(\left[0, 5\right]\)).</p> <p><img align="center" src="/assets/img/media/binet-0-5.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>Notice that the only places where Binet’s formula has real outputs on this interval are at the natural numbers, where the outputs are the typical Fibonacci numbers. What about the “negative Fibonacci numbers”? Let’s see what the outputs of Binet’s formula look like on the interval \(\left[-5, 0\right]\).</p> <p><img align="center" src="/assets/img/media/binetnegative.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>We end up with \(F_{-1} = 1, F_{-2} = -1, F_{-3} = 2, F_{-4} = -3\) … This large spiral that’s travelling around the complex plane actually intersects the real line at the usual Fibonacci numbers with alternating signs! There is actually a generalization of the typical recurrence relation that allows us to have negative values for \(n\):</p> \[F_{-n} = \left(-1\right)^{n+1}F_{n}\] <p>Extending discrete mathematical structures, such as the Fibonacci sequence, to have continuous properties often leads to interesting results. In this example, we saw how Binet’s formula allows us to find complex and negative “Fibonacci numbers”. The field of math that seeks to solve discrete problems about integers using tools from analysis is known as <em>analytic number theory</em>, and it has provided number theorists with other interesting results, such as bounds for the prime counting function and solutions to Diophantine equations.</p>]]></content><author><name></name></author><category term="number"/><category term="theory"/><summary type="html"><![CDATA[Binet’s Formula: \(F_{n} = \frac{1}{\sqrt{5}} \left(\left(\frac{1 + \sqrt{5}}{2}\right) ^{n} - \left(\frac{1 - \sqrt{5}}{2}\right)^{n}\right)\)]]></summary></entry><entry><title type="html">Computing the Cyclic Decomposition and Order of Elements from Symmetric Groups</title><link href="https://johnmath.github.io/blog/2020/compute_order_dfs/" rel="alternate" type="text/html" title="Computing the Cyclic Decomposition and Order of Elements from Symmetric Groups"/><published>2020-09-10T00:35:00+00:00</published><updated>2020-09-10T00:35:00+00:00</updated><id>https://johnmath.github.io/blog/2020/compute_order_dfs</id><content type="html" xml:base="https://johnmath.github.io/blog/2020/compute_order_dfs/"><![CDATA[<h2 id="math-background-and-introduction">Math Background and Introduction</h2> <p>I am currently taking an introductory class in abstract algebra, and we have been learning about different types of groups. One of these groups is called the <strong>Symmetric Group</strong>. The symmetric group defined over any set , \(\Omega\), is denoted as \(S_{\Omega}\). This group is comprised of all of the bijections, \(\sigma : \Omega \rightarrow \Omega\). of the set onto itself, and its group operation is defined as the composition of these bijections. Since we will be looking at finite symmetric groups, we can denote the symmetric group over a finite set of \(n\) symbols as \(S_{n}\).</p> <p>An example of an element in \(S_{10}\) could be the permutation (map) \(\sigma\) that rearranges [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] to be [3, 4, 6, 8, 10, 7, 9, 2, 1, 5].</p> <p>(i.e. \(\sigma([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) = [3, 4, 6, 8, 10, 7, 9, 2, 1, 5]\))</p> <p>Now that we know how the elements of \(S_{n}\) act on the underlying set, what are their cyclic decompositions and orders? The <strong>order</strong> of an element in a group refers to the smallest positive integer, \(m\) such that \(\sigma^{m} = \sigma \circ \sigma \circ ... \circ \sigma = \textbf{id}\) where <strong>id</strong> is that group’s identity element. The <strong>cyclic decomposition</strong> of one of these group elements refers to the “cycles” formed when repeatedly applying the same permutation on the underlying set. In other words, the cyclic decomposition refers to the “path” that each individual set element takes under a repeated permutation to get mapped back to itself.</p> <p>Viewing the permutation as a mapping of individual elements instead of a rearrangement of the entire set can aid in understanding how cyclic decompositions and repeated permutations work. Using the \(\sigma\) that we defined above, we can write out the following mapping:</p> \[1 \mapsto 3\] \[2 \mapsto 4\] \[3 \mapsto 6\] \[4 \mapsto 8\] \[5 \mapsto 10\] \[6 \mapsto 7\] \[7 \mapsto 9\] \[8 \mapsto 2\] \[9 \mapsto 1\] \[10 \mapsto 5\] <p>If we repeat this individual mapping repeatedly, we will eventually encounter elements that map back to their original positions. While sitting in class, it became apparent that this process could be automated using a directed graph and a depth-first search algorithm. The nodes of the graph would represent the set elements and the edges would represent their mapping under \(\sigma\). When the graph is drawn, the cyclic decompositions become obvious. The directed graph representing our \(\sigma\) on the set of 10 symbols is the following:</p> <p><img src="/assets/img/media/digraph_cyclic.jpg" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>Now that we can see the cycles in the form of a directed graph, let’s take a look at the code that would allow us to generalize the process of finding the cyclic decomposition adn order of any permutaion.</p> <h2 id="code">Code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="kn">from</span> <span class="n">math</span> <span class="kn">import</span> <span class="n">gcd</span>
</code></pre></div></div> <p>Firstly, we can use a dictionary to stand in as our \(\sigma\) as dictionaries have keys that map to values. Since our \(\sigma\) is bijective, a dictionary with unique (key, value) pairs is precisely what we need.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">sigma</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">:</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">:</span> <span class="mi">5</span><span class="p">}</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="nf">dict_keys</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="nf">dict_values</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div> <p>Now, we use the following algorithm to find the cyclic decomposition of \(\sigma\):</p> <p><em>1:</em> Instantiate an array, <code class="language-python highlighter-rouge"><span class="n">cycles</span></code> to store cycles and a set, <code class="language-python highlighter-rouge"><span class="n">already_seen</span></code> to store elements that have been encountered</p> <p><em>2:</em> Iterate over the values of the underlying set</p> <ul> <li> <p><strong>IF</strong> the current value is not in <code class="language-python highlighter-rouge"><span class="n">already_seen</span></code>, use DFS to repeat \(\sigma\) until the value is repeated.</p> <ul> <li>Append every element seen to <code class="language-python highlighter-rouge"><span class="n">cycles</span></code> and update <code class="language-python highlighter-rouge"><span class="n">already_seen</span></code> to include these elements.</li> </ul> </li> </ul> <p><em>3:</em> Return <code class="language-python highlighter-rouge"><span class="n">cycles</span></code></p> <p>In Python code, this algorithm would be</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">find_cyles</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Find cycles of a map using a depth first search</span><span class="sh">"""</span>
    
    <span class="n">cycles</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">already_seen</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">sigma</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">element</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">already_seen</span><span class="p">:</span>
            <span class="n">cycles</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">dfs</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">element</span><span class="p">,</span> <span class="nf">set</span><span class="p">(),</span> <span class="p">[]))</span>
            <span class="n">already_seen</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">cycles</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">cycles</span>
    
<span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">element</span><span class="p">,</span> <span class="n">memo</span><span class="p">,</span> <span class="n">cycle</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DFS Helper Function</span><span class="sh">"""</span>
    
    <span class="k">if</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
        <span class="k">return</span>
    
    <span class="n">memo</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
    <span class="nf">dfs</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma</span><span class="p">[</span><span class="n">element</span><span class="p">],</span> <span class="n">memo</span><span class="p">,</span> <span class="n">cycle</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>
</code></pre></div></div> <p><strong>DFS (depth-first search)</strong> is a graph traversal algorithm that starts at a “root” node and explores as far down that each of root’s branches as possible before backtracking and moving onto the next branch. Below is a gif that shows how DFS traverses a graph (<a href="https://codeforces.com/blog/entry/68138"><strong>Source</strong></a>)</p> <p><img src="https://codeforces.com/predownloaded/8d/be/8dbe5d89e58b67f3d8e4d8e0e8eb3358ba921b28.png" alt="digraph"/></p> <p>To find the order, we can use a <a href="http://mathonline.wikidot.com/the-order-theorem-for-permutations">theorem</a> which states that the order, \(m\),of a permutation is the least common multiple of the lengths of each cycle. By using this theorem, we can use the result from our DFS and avoid having to use the brute-force solution where we would compose \(\sigma\) with itself until we map back to the original ordering of elements.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">find_order</span><span class="p">(</span><span class="n">cycle_list</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute LCM of Cycle Lengths</span><span class="sh">"""</span>
    
    <span class="n">cycle_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cycle_list</span><span class="p">]</span>
    
    <span class="n">lcm</span> <span class="o">=</span> <span class="n">cycle_lengths</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">length</span> <span class="ow">in</span> <span class="n">cycle_lengths</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">lcm</span> <span class="o">=</span> <span class="n">lcm</span><span class="o">*</span><span class="n">length</span><span class="o">//</span><span class="nf">gcd</span><span class="p">(</span><span class="n">lcm</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">lcm</span>
</code></pre></div></div> <p>Now that we have these Python functions, we can use them in conjunction to find the order and cyclic decomposition for any map that belongs to a finite symmetric group! (<strong>Note:</strong> the notation for the cyclic decomposition (1 3 6 7 9) (8 2 4) (10 5) refers to the disjoint cycles that are produced. 1 maps to 3, 3 maps to 6, and so on.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">cycles</span> <span class="o">=</span> <span class="nf">find_cyles</span><span class="p">(</span><span class="n">sigma</span><span class="p">)</span>

<span class="n">cycles_string</span> <span class="o">=</span> <span class="sh">''</span>

<span class="k">for</span> <span class="n">cycle</span> <span class="ow">in</span> <span class="n">cycles</span><span class="p">:</span>
    <span class="n">cycles_string</span> <span class="o">+=</span> <span class="nf">str</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">cycle</span><span class="p">))</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span> 

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">The Cyclic Decomposition of Sigma is {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">cycles_string</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="s">Sigma has order {}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="nf">find_order</span><span class="p">(</span><span class="n">cycles</span><span class="p">)))</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">The</span> <span class="n">Cyclic</span> <span class="n">Decomposition</span> <span class="n">of</span> <span class="n">Sigma</span> <span class="ow">is</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> 

<span class="n">Sigma</span> <span class="n">has</span> <span class="n">order</span> <span class="mi">30</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>Graphs are an extremely versitile tool that allow us to represent both abstract mathematical objects and physical networks as data in memory. By using DFS, we can traverse these graphs that represent the permutations of a set to learn more about their underlying structures… and we can automate some problems from our abstract algebra homework.</p>]]></content><author><name></name></author><category term="algorithms"/><summary type="html"><![CDATA[Math Background and Introduction]]></summary></entry><entry><title type="html">Reproducing ‘Does data interpolation contradict statistical optimality?’</title><link href="https://johnmath.github.io/blog/2020/interpolation-estimator/" rel="alternate" type="text/html" title="Reproducing ‘Does data interpolation contradict statistical optimality?’"/><published>2020-05-21T00:35:00+00:00</published><updated>2020-05-21T00:35:00+00:00</updated><id>https://johnmath.github.io/blog/2020/interpolation-estimator</id><content type="html" xml:base="https://johnmath.github.io/blog/2020/interpolation-estimator/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>During one of my biweekly research meetings, my group reviewed <em><a href="https://arxiv.org/abs/1806.09471">Does data interpolation contradict statistical optimality?</a></em> by Mikhail Belkin, Alexander Rakhlin, and Alexandre B, Tsybakov</p> <p>The aim of this paper was to show that interpolating training data can still lead to optimal results in nonparametric regression and prediction with square loss. Since the double descent phenomenon exhibits itself when the model capacity surpasses the “interpolation threshold”, I thought that reproducing the results from this paper would help me understand how a model interpolates data.</p> <p>[1]</p> <p><img src="/assets/img/media/double_descent.png" alt="" style="height: 75%; width: 75%; object-fit: contain"/>)</p> <h2 id="math-and-code">Math and Code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">numpy.linalg</span> <span class="k">as</span> <span class="n">lin</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">scipy.stats</span> <span class="k">as</span> <span class="n">stats</span>
</code></pre></div></div> <p>This paper takes a look at interpolation using the <strong>Nadaraya-Watson Estimator</strong>.</p> <p>Let \((X, Y)\) be a random pair on \(\mathbb{R}^{d} \times \mathbb{R}\) with distribution \(P_{XY}\), and let \(\mathbb{E}[Y \vert X = x]\) be the regression function.</p> <p>Given a sample \((X_{1}, Y_{1}),...,(X_{n}, Y_{n})\) drawn independently from \(P_{XY}\), we can approximate $f(x)$ using the Nadaraya-Watson Estimator where \(K: \mathbb{R}^{d} \rightarrow \mathbb{R}\) is a kernel function and \(h &gt; 0\) is a bandwidth</p> <p><img src="/assets/img/media/nw_estimator.png" alt="" style="height: 50%; width: 50%; object-fit: contain"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">):</span>
    <span class="n">cols</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>

        <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nc">K</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">h</span><span class="p">)))</span>


    <span class="n">Kx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">cols</span><span class="p">))</span>

    <span class="n">row_sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Kx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">Kx</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">result</span><span class="p">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p><strong>Note:</strong> Since we are dealing with singular kernels that approach infinity when their argument tends to zero, we will have to use a modified version</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">stats</span><span class="p">.</span><span class="n">norm</span><span class="p">.</span><span class="n">pdf</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>

        <span class="n">condition</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">for</span> <span class="n">boolean</span> <span class="ow">in</span> <span class="p">[</span><span class="n">k</span><span class="o">==</span><span class="mi">0</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]]:</span>
            <span class="k">if</span> <span class="n">boolean</span><span class="p">:</span>
                <span class="n">condition</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
                <span class="k">break</span>

        <span class="k">if</span> <span class="n">condition</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
            <span class="n">cols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nc">K</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">/</span><span class="n">h</span><span class="p">,</span> <span class="n">a</span><span class="p">)))</span>


    <span class="n">Kx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">column_stack</span><span class="p">(</span><span class="nf">tuple</span><span class="p">(</span><span class="n">cols</span><span class="p">))</span>

    <span class="n">row_sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">Kx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">Kx</span> <span class="o">/</span> <span class="n">row_sums</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">result</span><span class="p">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p>The two singular kernels we will be focusing on are:</p> <p><img src="/assets/img/media/sing_kernel_1.png" alt="" style="height: 50%; width: 50%; object-fit: contain"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">sing_kernel_1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="n">a</span>
</code></pre></div></div> <p>and</p> <p><img src="/assets/img/media/sing_kernel_2.png" alt="" style="height: 50%; width: 50%; object-fit: contain"/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">sing_kernel_2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="nf">return </span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="n">a</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</code></pre></div></div> <p>The data generating functions we will look at are</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">actual_regression_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p>and</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="k">def</span> <span class="nf">binary_classification</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([])</span>

    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">abs</span><span class="p">(</span><span class="n">element</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">.</span><span class="mi">4</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">outs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">outs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">outs</span>
</code></pre></div></div> <h3 id="some-notes">Some notes:</h3> <ul> <li>Because of input size mismatches, I used <code class="language-python highlighter-rouge"><span class="nf">abs</span><span class="p">()</span></code> instead of <code class="language-python highlighter-rouge"><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">()</span></code> since I pass in the elements as arrays, but they are really (x, y) which are both just 1 dimensional real numbers</li> <li>The indicator function on <code class="language-python highlighter-rouge"><span class="n">sing_kernel_1</span></code> gave me errors when I tried implementing it, so i removed it. The kernel’s singularity as the argument goes to infinity is still present.</li> <li>The data from the sinusoidal function (p.4 of the paper) does not seem to be randomly sampled, so I did not randomly sample it either</li> </ul> <h2 id="results">Results</h2> <h3 id="sine-curve-with-singular_kernel_1">Sine curve with <code class="language-python highlighter-rouge"><span class="n">singular_kernel_1</span></code>:</h3> <p><img src="/assets/img/media/sine_nw_singular.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>The estimator fits the curve fairly well for values of <em>a</em> &gt; .8. For some reason, the bandwidth, <em>h</em>, doesn’t do anything to this specific example. Because the best value of <em>h</em> in the paper was .4, I chose to keep h constant.</p> <h3 id="sine-curve-with-standard-gaussian-kernel">Sine curve with standard Gaussian kernel:</h3> <p><img src="/assets/img/media/non_sing_sine.png" alt="" style="height: 60%; width: 60%; object-fit: contain"/></p> <p>There are no animations for different parameters because the only tunable parameter is the bandwidth, <em>h</em>, which was held constant at .4</p> <h4 id="code-from-notebook">Code (from notebook)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#np.random.seed(100)
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">#epsilon = np.random.normal(loc = 0, scale = 2, size = n)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1">#np.random.normal(loc = 0, scale = 3, size=n)
</span><span class="n">Y</span> <span class="o">=</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># Singular Kernel
</span><span class="n">h</span><span class="o">=</span><span class="p">.</span><span class="mi">4</span>
<span class="n">a</span><span class="o">=</span><span class="mf">1.5</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">x_axis</span><span class="p">),</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">sing_kernel_1</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">True Regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1"># Non-singular Kernel
</span>
<span class="n">h</span><span class="o">=</span><span class="n">n</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">actual_distribution_1</span><span class="p">(</span><span class="n">x_axis</span><span class="p">),</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">True Regression</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="binary-data-with-singular_kernel_2">Binary Data with <code class="language-python highlighter-rouge"><span class="n">singular_kernel_2</span></code>:</h3> <p><img src="/assets/img/media/binary_sing.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>In this animation, I sweep through several values for <em>h</em> with <em>a</em> constant. Then I hold <em>h</em> constant and sweep through several values of <em>a</em></p> <h3 id="binary-data-with-standard-gaussian-kernel">Binary data with standard Gaussian kernel</h3> <p><img src="/assets/img/media/non_sing_binary.gif" alt="" style="height: 75%; width: 75%; object-fit: contain"/></p> <p>The only tunable parameter here is <em>h</em></p> <h4 id="code-from-notebook-1">Code (from notebook)</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="c1">#np.random.seed(100)
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">#epsilon = np.random.normal(loc = 0, scale = 2, size = n)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="nf">binary_distribution</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">h</span><span class="o">=</span><span class="n">n</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)))</span>
<span class="n">a</span><span class="o">=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)),</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">singular_nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="n">sing_kernel_2</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">a = {:.2f}; h = {:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Boundary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="syntax"><code><span class="n">h</span> <span class="o">=</span><span class="p">.</span><span class="mi">4</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_axis</span><span class="p">)),</span> <span class="sh">'</span><span class="s">b--</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="nf">nadaraya_watson_estimator</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">-</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">h = {:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">([</span><span class="sh">'</span><span class="s">Boundary</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Estimator</span><span class="sh">'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>After reproducing these results, I emailed Mikhail Belkin to get his thoughts on the connection between this paper and a previous paper he wrote on the double descent phenomenon. His reply was very similar to what my group thought the connection may be: <strong>interpolation is consistent with the current practice of deep learning.</strong> The most important thing to realize is that there is still not a proven, complete connection between modern machine learning methods and the model shown in this post.</p> <p>References:</p> <ul> <li>Does data interpolation contradict statistical optimality? - <a href="https://arxiv.org/abs/1806.09471">https://arxiv.org/abs/1806.09471</a></li> <li>[1] - <a href="https://www.cs.ubc.ca/labs/lci/mlrg/slides/dl_generalization.pdf">https://www.cs.ubc.ca/labs/lci/mlrg/slides/dl_generalization.pdf</a></li> </ul>]]></content><author><name></name></author><category term="machine"/><category term="learning"/><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>